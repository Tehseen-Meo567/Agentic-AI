{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LangChain**\n",
        "Definition: Framework for developing applications powered by large language models (LLMs).\n",
        "\n",
        "1.   Development: Build your applications using its own open-source components+third-party integrations.**LangGraph** - build stateful agents with first-class streaming and human-in-the-loop support.\n",
        "2.   Productionization\n",
        "3.   Deployment"
      ],
      "metadata": {
        "id": "drWWcVFQ5cux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Setup Environment/ Install Libraries**"
      ],
      "metadata": {
        "id": "UK8ufKHsVqvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain_google_genai"
      ],
      "metadata": {
        "id": "S8VpfxO1M0El",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cdef98c-45a9-4d48-906e-c644dd6305b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Configure API Key and Gemini Flash Model 2.0**"
      ],
      "metadata": {
        "id": "p5Zz2JxcWQ1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "api_key=userdata.get('GOOGLE_API_KEY')\n",
        "model= ChatGoogleGenerativeAI(api_key=api_key,model=\"gemini-2.0-flash-exp\")\n"
      ],
      "metadata": {
        "id": "CosBDt4AM8zB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Create a Prompt Template**"
      ],
      "metadata": {
        "id": "jI2OF28RW7MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "OaCm4VG_OyCx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"I want you to answer only math related question and don't respond if question is from other subject:\\n\\n{question}\"\n",
        ")"
      ],
      "metadata": {
        "id": "m47hUdLsNmp3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Build the LangChain Pipeline**"
      ],
      "metadata": {
        "id": "Wwo2heZCXHPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the LLM chain\n",
        "chain = LLMChain(llm=model, prompt=prompt_template)"
      ],
      "metadata": {
        "id": "dtlR0-q1XL5g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Run the Math Solver Agent**"
      ],
      "metadata": {
        "id": "ILbNYJGnXNDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the chain with a sample question\n",
        "chain.invoke(\"tell me factorization of this equation: x^2+5x+6?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5b34ada-98e6-4e95-d7b7-48b129bf9f93",
        "id": "dMoiOVZVYb33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'tell me factorization of this equation: x^2+5x+6?',\n",
              " 'text': '(x + 2)(x + 3)\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the chain with a sample question\n",
        "chain.invoke(\"What is Open Loop System?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c386eaa9-7df9-4023-ae47-4b9f69efdb12",
        "id": "r-6W8XyDYiFM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is Open Loop System?',\n",
              " 'text': 'I cannot answer this question as it is not related to mathematics.\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}